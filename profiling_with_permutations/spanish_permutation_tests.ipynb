{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#import torch\n",
    "#from transformers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.interpolate.interpnd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:15:14.820153Z",
     "start_time": "2022-03-29T14:15:14.732732Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_grammar(sent,target_words):\n",
    "    morph_properties = {w: {} for w in target_words}\n",
    "    syntax_properties = {w: {} for w in target_words}\n",
    "    for line in sent:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if line.startswith(\"# \"):\n",
    "            continue\n",
    "        (\n",
    "            identifier,\n",
    "            form,\n",
    "            lemma,\n",
    "            pos,\n",
    "            xpos,\n",
    "            feats,\n",
    "            head,\n",
    "            rel,\n",
    "            enh,\n",
    "            misc,\n",
    "        ) = line.strip().split(\"\\t\")\n",
    "        if lemma in target_words:\n",
    "            if target_words[lemma]:\n",
    "                if pos != target_words[lemma]:\n",
    "                    continue\n",
    "            if feats not in morph_properties[lemma]:\n",
    "                morph_properties[lemma][feats] = 0\n",
    "            morph_properties[lemma][feats] += 1\n",
    "            if rel not in syntax_properties[lemma]:\n",
    "                syntax_properties[lemma][rel] = 0\n",
    "            syntax_properties[lemma][rel] += 1\n",
    "    morph_properties = [{key:val} for key, val in morph_properties.items() if val]\n",
    "    syntax_properties = [{key:val} for key, val in syntax_properties.items() if val]\n",
    "\n",
    "    return(morph_properties, syntax_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:15:15.748846Z",
     "start_time": "2022-03-29T14:15:15.536643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent_profiles(file, target_words):\n",
    "    scores = []\n",
    "    scores_file = open(file, 'r', encoding='utf8')\n",
    "    line_list = list(scores_file.readlines())\n",
    "    i = 0\n",
    "    while i < len(line_list):\n",
    "        scores.append(line_list[i])\n",
    "        i += 1\n",
    "    sents=[]\n",
    "    for i,line in enumerate(scores):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"# \") and line.find('sent_id')!=-1:\n",
    "            sent_id = line.split()[-1]\n",
    "            sents.append(i)\n",
    "    scores_arr  = []\n",
    "    idx = 0\n",
    "    while idx+1 < len(sents):\n",
    "        cur_val = sents[idx]\n",
    "        next_val = sents[idx+1]\n",
    "        sent = scores[cur_val+2:next_val]\n",
    "        scores_arr.append(sent)\n",
    "        idx+=1\n",
    "    sent_profiles = {}\n",
    "    for i,sent in enumerate(scores_arr):\n",
    "        morph_properties, syntax_properties = make_grammar(sent, target_words)\n",
    "        if morph_properties:\n",
    "            sent_profiles['1.'+str(i+1)] = morph_properties\n",
    "    word_stat = {w: {'sents':[],'features':{}} for w in target_words}\n",
    "    for idx, num in enumerate(sent_profiles):\n",
    "        profile = sent_profiles[num]\n",
    "        for prof in profile:\n",
    "                for key, value in prof.items():\n",
    "                    for val in value:\n",
    "                        if word_stat[key]['features'].get(val):\n",
    "                            word_stat[key]['features'][val]+=1\n",
    "                            word_stat[key]['sents'].append(num)\n",
    "                        else:\n",
    "\n",
    "                            word_stat[key]['features'][val]=1\n",
    "                            word_stat[key]['sents'].append(num)\n",
    "    sent_data = {w:word_stat[w]['sents'] for w in word_stat}\n",
    "    return sent_data, sent_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:23:46.600973Z",
     "start_time": "2022-03-29T14:15:52.069694Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_sent_data, old_sent_profiles = get_sent_profiles('kk_parsed_old_ES.conllu', target_words)\n",
    "modern_sent_data, modern_sent_profiles = get_sent_profiles('kk_parsed_modern_ES.conllu', target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:00.455715Z",
     "start_time": "2022-03-29T14:23:47.735573Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_chars = set(string.punctuation) - set(\"_\")\n",
    "\n",
    "def clean(s) :\n",
    "    global bad_chars\n",
    "\n",
    "    #s = s.replace('_nn','').replace('_vb','')\n",
    "\n",
    "    if \"'\" in s :\n",
    "        return \"\" # ignore contractions\n",
    "    if \"â€™\" in s :\n",
    "        return \"\" # ignore contractions\n",
    "    #if \"-\" in s :\n",
    "    #    return \"\" # ignore hyphenated words\n",
    "    if s != 'f5' :\n",
    "        for n in string.digits :\n",
    "            if n in s :\n",
    "                return \"\" # ignore anything with a number\n",
    "    return ''.join([ c if c not in bad_chars else '' for c in s ])\n",
    "\n",
    "# returns [(string,[w0,w1,...wN]),(string,[w0,w1,...wN]),...]\n",
    "def read_data(fname) :\n",
    "    sentences = []\n",
    "    count = 0\n",
    "    rejected = 0\n",
    "    stop = False\n",
    "    print(\"reading {} ...\".format(fname))\n",
    "    with open(fname, encoding=\"utf8\") as f :\n",
    "        for line in f :\n",
    "            line = line.strip()\n",
    "            if not line : continue\n",
    "            #for sent in sent_tokenize(line) :\n",
    "            if 1 :\n",
    "                sent = line\n",
    "                words = sent.split()\n",
    "                #if len(words) < 5 :\n",
    "                #    rejected += 1\n",
    "                #    continue\n",
    "                words = [ clean(w) for w in words ]\n",
    "                words = set([ w for w in words if (w != '') ])\n",
    "                sentences.append((sent,words))\n",
    "                count += 1\n",
    "    print(\"  - read {} sentences (rejected {})\".format(count, rejected))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def process_sentences(corpus, tokenizer, word_list) :\n",
    "    for text,words in corpus :\n",
    "        if not any([ w in words for w in word_list]) :\n",
    "            continue\n",
    "\n",
    "        text = text.replace('_nn', '').replace('_vb', '')\n",
    "\n",
    "        tokens = tokenizer.tokenize(\"[CLS] \" + text + \" [SEP]\")\n",
    "        tokens = [ t for t in tokens if t != '[UNK]' ] # emoji are [UNK]\n",
    "        if len(tokens) > 512 :\n",
    "            continue\n",
    "        yield tokens,tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def find_tokens2(tokens, words_list) :\n",
    "    tmp = []\n",
    "    word = \"\"\n",
    "    offset = 1 # we need to see a word without '##', so offset is always at least 1\n",
    "\n",
    "    for i in range(len(tokens)) :\n",
    "        current = tokens[i]\n",
    "        if current.startswith(\"##\") :\n",
    "            word += current.replace(\"##\", \"\")\n",
    "            offset += 1\n",
    "        else :\n",
    "            if word and (word in words_list) :\n",
    "                tmp.append((i-offset, word))\n",
    "            offset = 1\n",
    "            word = current\n",
    "\n",
    "    if word and (word in words_list) :\n",
    "        tmp.append((i-offset, word))\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def mk_batch(tokens_list) :\n",
    "    mx = max([ len(t) for t in tokens_list ])\n",
    "    tokens = np.array([ t + ([0] * (mx-len(t))) for t in tokens_list ])\n",
    "    mask = np.where(tokens != 0, 1, 0)\n",
    "    return torch.LongTensor(tokens), torch.LongTensor(mask)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def process_batch(token_batch, ids_batch, model, tokenizer, words_counters) :\n",
    "    tokens,segments = mk_batch(ids_batch)\n",
    "    tokens = tokens.to(\"cuda\")      # gpu\n",
    "    segments = segments.to(\"cuda\")  # gpu\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        _, _, hidden_states = model(tokens, segments)\n",
    "\n",
    "    hidden_states = torch.stack(hidden_states, dim=0)\n",
    "    hidden_states = hidden_states.permute(1,2,0,3)\n",
    "\n",
    "    embeddings = []\n",
    "    # sum the last 4 hidden layers\n",
    "    # to extract sub-word embeddings\n",
    "    for index,sentence in enumerate(hidden_states) :\n",
    "        for i,w in  find_tokens2(token_batch[index], words_counters) :\n",
    "            wt = tokenizer.tokenize(w)\n",
    "\n",
    "            sum_vec = torch.sum(sentence[i][-4:], dim=0)\n",
    "\n",
    "            for j in range(1, len(wt)) :\n",
    "                sum_vec += torch.sum(sentence[i+j][-4:], dim=0)\n",
    "\n",
    "            sum_vec /= len(wt)\n",
    "            sum_vec = sum_vec.cpu().numpy()\n",
    "\n",
    "            embeddings.append((w, sum_vec))\n",
    "            words_counters[w] -= 1\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def avg_embeddings(emb) :\n",
    "    return np.sum(emb, axis=0) / len(emb)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def get_wordcount(sentences) :\n",
    "    c = Counter()\n",
    "    for sentence in sentences :\n",
    "        text,words = sentence\n",
    "        c.update(words)\n",
    "    return c\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_wordlist(fname) : #wc1, wc2) :\n",
    "    #return list(set(wc1).intersection(set(wc2)))\n",
    "    tmp = []\n",
    "    with open(fname) as f :\n",
    "        f.readline() # header\n",
    "        for line in f :\n",
    "            line = line.strip().split(',')[1]\n",
    "            tmp.append(line)\n",
    "    return tmp\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def extract_embeddings(sentences, word_list, max_word, emb_fname, model, tokenizer, max_batch) :\n",
    "    random.shuffle(sentences)\n",
    "    words_counters = dict([ (w.split('_')[0], max_word) for w in word_list ])\n",
    "    token_batch = []\n",
    "    ids_batch = []\n",
    "    embeddings = []\n",
    "    count = 0\n",
    "    total_words = len(word_list)\n",
    "    \n",
    "    for tokens,ids in process_sentences(sentences, tokenizer, word_list) :\n",
    "        token_batch.append(tokens)\n",
    "        ids_batch.append(ids)\n",
    "        \n",
    "        if len(token_batch) < max_batch :\n",
    "            continue\n",
    "        \n",
    "        embeddings.extend(process_batch(token_batch, ids_batch, model, tokenizer, words_counters))\n",
    "        count += 1\n",
    "        token_batch = []\n",
    "        ids_batch = []\n",
    "        \n",
    "        to_delete = []\n",
    "        for w in words_counters :\n",
    "            if words_counters[w] <= 0 :\n",
    "                to_delete.append(w)\n",
    "        for w in to_delete :\n",
    "            del words_counters[w]\n",
    "        print(\"\\rbatch {} - progress={}/{}\".format(count, len(words_counters), total_words), end=\"\", flush=True)\n",
    "        \n",
    "    if len(token_batch) > 0 :\n",
    "        embeddings.extend(process_batch(token_batch, ids_batch, model, tokenizer, words_counters))\n",
    "    \n",
    "    print(\"done!\")\n",
    "\n",
    "    print(\"\\nsyncing to disk...\")\n",
    "    with open(emb_fname, 'wb') as f :\n",
    "        pickle.dump(embeddings, f)\n",
    "        \n",
    "    print(\"done!\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def load_embeddings(fname) :\n",
    "    with open(fname, 'rb') as f :\n",
    "        tmp = pickle.load(f)\n",
    "        \n",
    "    tmp2 = defaultdict(list)\n",
    "    for word,vector in tmp :\n",
    "        tmp2[word].append(np.array(vector))\n",
    "        \n",
    "    print(\"read {} embeddings for {} words ({})\".format(len(tmp), len(tmp2), fname))\n",
    "    \n",
    "    for word,vectors in tmp2.items() :\n",
    "        tmp2[word] = np.array(vectors)\n",
    "        \n",
    "    return tmp2\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def num_combinations(n, r) :\n",
    "    try :\n",
    "        return math.factorial(n) / (math.factorial(r) * math.factorial(n - r))\n",
    "    except :\n",
    "        return 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:01.219762Z",
     "start_time": "2022-03-29T14:24:01.207908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist_fname = 'target_words_evaluation_phase2.txt'\n",
    "\n",
    "#results_fname = '3_28.03_permutation_results.txt' # 'shift_simulation_6_results.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:01.712231Z",
     "start_time": "2022-03-29T14:24:01.698646Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': [],\n",
       " 'actitud': [],\n",
       " 'ataque': [],\n",
       " 'atrÃ¡s': [],\n",
       " 'ausencia': [],\n",
       " 'avance': [],\n",
       " 'banco': [],\n",
       " 'canal': [],\n",
       " 'capital': [],\n",
       " 'cobrar': [],\n",
       " 'colaborar': [],\n",
       " 'compasiÃ³n': [],\n",
       " 'copiar': [],\n",
       " 'corriente': [],\n",
       " 'cÃ³lera': [],\n",
       " 'declinar': [],\n",
       " 'demÃ¡': [],\n",
       " 'diligencia': [],\n",
       " 'disco': [],\n",
       " 'distribuir': [],\n",
       " 'educado': [],\n",
       " 'elocuente': [],\n",
       " 'encargado': [],\n",
       " 'enterar': [],\n",
       " 'especulaciÃ³n': [],\n",
       " 'fallar': [],\n",
       " 'fallecimiento': [],\n",
       " 'historia': [],\n",
       " 'historiador': [],\n",
       " 'impulso': [],\n",
       " 'indicativo': [],\n",
       " 'juguete': [],\n",
       " 'maduro': [],\n",
       " 'maravilloso': [],\n",
       " 'marco': [],\n",
       " 'matiz': [],\n",
       " 'metal': [],\n",
       " 'metro': [],\n",
       " 'modificado': [],\n",
       " 'mÃ©dula': [],\n",
       " 'nombrar': [],\n",
       " 'pendiente': [],\n",
       " 'pila': [],\n",
       " 'planta': [],\n",
       " 'prima': [],\n",
       " 'propiamente': [],\n",
       " 'prÃ³ximo': [],\n",
       " 'recomendar': [],\n",
       " 'recordar': [],\n",
       " 'retroceder': [],\n",
       " 'satÃ©lite': [],\n",
       " 'socialista': [],\n",
       " 'solicitud': [],\n",
       " 'susceptible': [],\n",
       " 'tarea': [],\n",
       " 'trato': [],\n",
       " 'tropical': [],\n",
       " 'variedad': [],\n",
       " 'viernes': [],\n",
       " 'visita': [],\n",
       " 'vuestro': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('target_words_evaluation_phase2.txt','r', encoding = 'utf-8').read().split('\\n')\n",
    "target_words = {w:[] for w in words}\n",
    "target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:02.869340Z",
     "start_time": "2022-03-29T14:24:02.824535Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_word_properties(properties):\n",
    "    props = defaultdict(lambda: defaultdict(int))\n",
    "    for features, count in properties.items():\n",
    "        separate_features = features.split(\"|\")\n",
    "        for feat in separate_features:\n",
    "            try:\n",
    "                k, v = feat.split(\"=\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            else:\n",
    "                props[k][v] += count\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:03.758598Z",
     "start_time": "2022-03-29T14:24:03.719691Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_distance_from_common_features(p1, p2, threshold, distance_type):\n",
    "    features = find_features(p1, p2, threshold)\n",
    "    vector_1, vector_2 = make_vectors(features, p1, p2)\n",
    "    return compute_distance(vector_1, vector_2, distance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:04.683512Z",
     "start_time": "2022-03-29T14:24:04.662669Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vectors(features, p1, p2):\n",
    "    vector_1 = np.zeros(len(features))\n",
    "    vector_2 = np.zeros(len(features))\n",
    "\n",
    "    for nr, feature in enumerate(features):\n",
    "        vector_1[nr] = p1.get(feature, 0)\n",
    "        vector_2[nr] = p2.get(feature, 0)\n",
    "\n",
    "    return vector_1, vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:05.618968Z",
     "start_time": "2022-03-29T14:24:05.588405Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_distance(vector_1, vector_2, distance_type):\n",
    "    if distance_type == \"cos\":\n",
    "        dist = cosine(vector_1, vector_2)\n",
    "        if np.isnan(dist):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return dist\n",
    "    elif distance_type == \"jsd\":\n",
    "        return jensenshannon(vector_1, vector_2)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown distance: {distance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:06.593674Z",
     "start_time": "2022-03-29T14:24:06.544449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(p1, p2, threshold):\n",
    "    features = list(p1.keys() | p2.keys())\n",
    "    prop_count = {k: p1.get(k, 0) + p2.get(k, 0) for k in features}\n",
    "    total = sum(prop_count.values())\n",
    "    return [f for f in features if prop_count[f] / total * 100 > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:07.662682Z",
     "start_time": "2022-03-29T14:24:07.433010Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_1 = json.load(open('kk_old_ES_morph.json', \"r\", encoding = 'utf-8'))\n",
    "properties_2 = json.load(open('kk_modern_ES_morph.json', \"r\", encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:08.895328Z",
     "start_time": "2022-03-29T14:24:08.780790Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def permutation_test(g1, g2, dist,word) :\n",
    "    top_half = len(g1)\n",
    "    g = np.concatenate((g1, g2), axis=0)\n",
    "    tmp = []\n",
    "    if num_combinations(len(g), top_half) < 1000 :\n",
    "        return dist,exact_pval(g1,g2,dist,word)\n",
    "\n",
    "    else :\n",
    "        for _ in range(1000) :\n",
    "            \n",
    "            np.random.shuffle(g)\n",
    "            perm1 = g[: top_half ]\n",
    "            perm2 = g[top_half : ]\n",
    "            prof1 =get_new_stats(perm1,word)\n",
    "            prof2 = get_new_stats(perm2,word)\n",
    "            tmp.append(find_dist(prof1,prof2))\n",
    "\n",
    "        pval = sum([ 1 for i in tmp if i >= dist ]) / len(tmp)\n",
    "        if pval > 0.05 :\n",
    "            return dist,pval\n",
    "\n",
    "        if num_combinations(len(g), top_half) < 10000 :\n",
    "            return dist,exact_pval(g1,g2,dist,word) \n",
    "        for _ in range(9000) :\n",
    "            \n",
    "            np.random.shuffle(g)\n",
    "            perm1 = g[: top_half ]\n",
    "            perm2 = g[top_half : ]\n",
    "            prof1 =get_new_stats(perm1,word)\n",
    "            prof2 = get_new_stats(perm2,word)\n",
    "            tmp.append(find_dist(prof1,prof2))\n",
    "\n",
    "        pval = sum([ 1 for i in tmp if i >= dist ]) / len(tmp)\n",
    "        if pval > 0.005 :\n",
    "            return dist,pval\n",
    "\n",
    "\n",
    "        if num_combinations(len(g), top_half) < 100000 :\n",
    "            return dist,exact_pval(g1,g2,dist,word)\n",
    "        for _ in range(90000) :\n",
    "            np.random.shuffle(g)\n",
    "            perm1 = g[: top_half ]\n",
    "            perm2 = g[top_half : ]\n",
    "            prof1 =get_new_stats(perm1,word)\n",
    "            prof2 = get_new_stats(perm2,word)\n",
    "            tmp.append(find_dist(prof1,prof2))\n",
    "\n",
    "        pval = sum([ 1 for i in tmp if i >= dist ]) / len(tmp)\n",
    "        if pval == 0.0 :\n",
    "            pval = 1 / 100000.0\n",
    "        return dist,pval \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:10.126173Z",
     "start_time": "2022-03-29T14:24:10.071306Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exact_pval(g1, g2, dist,word) :\n",
    "    #print('exact_pval')\n",
    "    top_half = len(g1)\n",
    "    g = np.concatenate((g1, g2), axis=0)\n",
    "    tmp = []\n",
    "    #print('num itertools: ',len(list(itertools.combinations(range(len(g)), top_half))))\n",
    "    #print('comb_example: ', list(itertools.combinations(range(len(g)), top_half))[0])\n",
    "    for ordering in itertools.combinations(range(len(g)), top_half) :\n",
    "        perm1 = np.take(g, ordering, axis=0)\n",
    "        perm2 = np.delete(g, ordering, axis=0)\n",
    "        prof1 =get_new_stats(perm1,word)\n",
    "        prof2 = get_new_stats(perm2,word)\n",
    "        tmp.append(find_dist(prof1,prof2)) \n",
    "    pval = sum([ 1 for i in tmp if i >= dist ]) / len(tmp)\n",
    "    #print('pval: ', pval)\n",
    "    return pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:11.223264Z",
     "start_time": "2022-03-29T14:24:11.190482Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_separation(word_properties):\n",
    "    properties = defaultdict(int)\n",
    "    for el in word_properties:\n",
    "        for feat in el.split(\"|\"):\n",
    "            properties[feat] += word_properties[el]\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:12.312001Z",
     "start_time": "2022-03-29T14:24:12.250228Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def get_new_stats(perm2,word):\n",
    "    #print('get_new_stats for ', perm2)\n",
    "    feature_arr = []\n",
    "    for sent in perm2:\n",
    "        profile_dic = {}\n",
    "        corp = sent.split('.')[0]\n",
    "        if corp == '1':\n",
    "            sentence = old_sent_profiles[sent]\n",
    "            for w in sentence:\n",
    "                if word in w.keys():\n",
    "                    features = w[word]\n",
    "                    feature_arr.append(features)\n",
    "        if corp == '2':\n",
    "            sentence = modern_sent_profiles['1.'+ sent.split('.')[1]]\n",
    "            for w in sentence:\n",
    "                if word in w.keys():\n",
    "                    features = w[word]\n",
    "                    feature_arr.append(features)\n",
    "    #print('feature_arr', feature_arr)\n",
    "                \n",
    "    sep_arr = []\n",
    "    for f in feature_arr:\n",
    "        f_sep = feature_separation(f)\n",
    "        sep_arr.append(f_sep)\n",
    "    #print('sep_arr ', sep_arr)\n",
    "    common_features = []\n",
    "    for s in sep_arr:\n",
    "        common_features.extend(list(s.keys()))\n",
    "    common_features = set(common_features)\n",
    "    #print('common_features ', common_features)\n",
    "    new_dic = {}\n",
    "    for f in common_features:\n",
    "        res = 0\n",
    "        for sep in sep_arr:\n",
    "            res+=sep.get(f,0)\n",
    "        new_dic[f] = res\n",
    "    #print('\\n\\n\\n',new_dic,'\\n\\n\\n')\n",
    "    return new_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T14:24:13.358160Z",
     "start_time": "2022-03-29T14:24:13.304362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_dist(p1,p2):\n",
    "    distance = {}\n",
    "    p1 = collect_word_properties(p1)\n",
    "    p2 = collect_word_properties(p2)\n",
    "    #print('p1', p1)\n",
    "    #print('p2', p2)\n",
    "    #print('\\n')\n",
    "    feature_classes = list(p1.keys() | p2.keys())\n",
    "    \n",
    "    for f_class in feature_classes:\n",
    "        #print('f_class ',f_class)\n",
    "        #print('\\n')\n",
    "        distance[f_class] = \\\n",
    "            compute_distance_from_common_features(p1[f_class], p2[f_class], 0, 'cos')\n",
    "        #print(distance[f_class])\n",
    "        #print('\\n')\n",
    "    distance = [d for d in distance.values() if not np.isnan(d)]\n",
    "    avg_dist = np.mean(distance)\n",
    "    \n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-29T14:32:41.890Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating statistics ...\n",
      "  - actitud (0 / 61)          \n",
      "actitud 906 1860 0.007469774058661005 1e-05\n",
      "  - ataque (1 / 61)          \n",
      "ataque 627 2857 0.11245526773715697 1e-05\n",
      "  - atrÃ¡s (2 / 61)          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ð¡Ð²ÐµÑ‚Ð»Ð°Ð½Ð°\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\spatial\\distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atrÃ¡s 1365 1324 0.0 1.0\n",
      "  - ausencia (3 / 61)          \n",
      "ausencia 660 935 0.002962396133941003 1e-05\n",
      "  - avance (4 / 61)          \n",
      "avance 86 3509 0.043536073863454296 1e-05\n",
      "  - banco (5 / 61)          \n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "data = {}\n",
    "print(\"calculating statistics ...\")\n",
    "for idx,w in enumerate(words):   \n",
    "    distance = {}\n",
    "    total_words = len(words)\n",
    "    print(\"  - {} ({} / {})          \".format(w, idx, total_words))\n",
    "    \n",
    "    p1 = collect_word_properties(properties_1[w])\n",
    "    p2 = collect_word_properties(properties_2[w])\n",
    "    #print('p1:',p1,'\\np2:',p2)\n",
    "    feature_classes = list(p1.keys() | p2.keys())\n",
    "    #print(feature_classes)\n",
    "    for f_class in feature_classes:\n",
    "        distance[f_class] = \\\n",
    "            compute_distance_from_common_features(p1[f_class],\n",
    "                                                  p2[f_class],\n",
    "                                                  0,\n",
    "                                                  'cos')\n",
    "        #print('dist',distance[f_class] )\n",
    "    distance = [d for d in distance.values() if not np.isnan(d)]\n",
    "    avg_dist = np.mean(distance)\n",
    "    #print('avg dist: ', avg_dist)\n",
    "    sents1 = old_sent_data[w]\n",
    "    sents2 = modern_sent_data[w]\n",
    "    sents2 =['2.'+i.split('.')[1] for i in sents2]\n",
    "\n",
    "    permutation_dist, permutation_pval = permutation_test(sents1, sents2, avg_dist,w)\n",
    "\n",
    "    print(w, len(sents1), len(sents2), permutation_dist, permutation_pval)\n",
    "    data[w] = (permutation_dist, permutation_pval)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:20:41.030313Z",
     "start_time": "2022-03-29T07:20:34.396436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['word', 'dist', 'pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:21:03.393241Z",
     "start_time": "2022-03-29T07:21:01.957301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011144501557778241 [0.255875, 0.0053728, 1.0, 0.0031625, 1.0, 0.0, 0.27321212121212124, 1.0, 0.3246285714285714, 0.009907692307692308, 0.054518518518518515, 0.00012, 0.09528571428571428, 2.090909090909091e-05, 0.1426, 0.255875, 0.282764705882353, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1426]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "pvalues = [float(d[2]) for d in data]\n",
    "\n",
    "_,fdr,alpha,_ = multipletests(pvalues, method='fdr_bh')\n",
    "fdr = fdr.tolist()\n",
    "df['fdr'] = fdr\n",
    "print(alpha, fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:21:07.215910Z",
     "start_time": "2022-03-29T07:21:07.190296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dist'] = df['dist'].astype('float')\n",
    "df['pval'] = df['pval'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:21:09.159971Z",
     "start_time": "2022-03-29T07:21:09.100964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graded = {}\n",
    "with open(\"graded_de_true.txt\", encoding = 'utf-8') as f :\n",
    "    f.readline()\n",
    "    for line in f :\n",
    "        line = line.strip()\n",
    "        if not line : continue\n",
    "        word, shift = line.split()\n",
    "        graded[word] = float(shift)\n",
    "arr = []\n",
    "for w in df.word:\n",
    "    arr.append(graded[w])\n",
    "df['shift'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:21:10.069775Z",
     "start_time": "2022-03-29T07:21:10.037121Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = {}\n",
    "with open(\"binary_de_true.txt\", encoding = 'utf-8') as f :\n",
    "    f.readline()\n",
    "    for line in f :\n",
    "        line = line.strip()\n",
    "        if not line : continue\n",
    "        word, shift = line.split()\n",
    "        binary[word] = float(shift)\n",
    "arr = []\n",
    "for w in df.word:\n",
    "    arr.append(binary[w])\n",
    "df['bin'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T07:22:06.268042Z",
     "start_time": "2022-03-29T07:22:06.211521Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mix'] = df.apply(lambda x: x.dist*x.fdr, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T02:20:43.990633Z",
     "start_time": "2022-03-29T02:20:43.880745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "      <th>pval</th>\n",
       "      <th>fdr</th>\n",
       "      <th>shift</th>\n",
       "      <th>bin</th>\n",
       "      <th>mix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pachtzins</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ausspannen</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706690</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sensation</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>vergÃ¶nnen</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.071197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Frechheit</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.192625</td>\n",
       "      <td>0.070839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>vorliegen</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ohrwurm</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.832451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Unentschlossenheit</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Titel</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.393045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FuÃŸ</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.293588</td>\n",
       "      <td>0.564633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>weitgreifend</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MiÃŸklang</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.379723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aufrechterhalten</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.036109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AckergerÃ¤t</td>\n",
       "      <td>0.017583</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.799889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.985935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Entscheidung</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.141681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>packen</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.462253</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Schmiere</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.437671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gesichtsausdruck</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.976257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ausnahmegesetz</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.302286</td>\n",
       "      <td>0.093138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tier</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Seminar</td>\n",
       "      <td>0.028230</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.064486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Knotenpunkt</td>\n",
       "      <td>0.029140</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.647627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mulatte</td>\n",
       "      <td>0.032976</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ã¼berspannen</td>\n",
       "      <td>0.033236</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abgesang</td>\n",
       "      <td>0.033390</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.578548</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Spielball</td>\n",
       "      <td>0.035748</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.103290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kubikmeter</td>\n",
       "      <td>0.037032</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Einreichung</td>\n",
       "      <td>0.037602</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.051093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TragfÃ¤higkeit</td>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.114694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armenhaus</td>\n",
       "      <td>0.040294</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.519670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>artikulieren</td>\n",
       "      <td>0.040389</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>voranstellen</td>\n",
       "      <td>0.042848</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.124192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dynamik</td>\n",
       "      <td>0.043546</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.578845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EngpaÃŸ</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.819957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Manschette</td>\n",
       "      <td>0.051829</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.355802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaturschÃ¶nheit</td>\n",
       "      <td>0.052340</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.071561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abgebrÃ¼ht</td>\n",
       "      <td>0.056371</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.210485</td>\n",
       "      <td>0.832645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Festspiel</td>\n",
       "      <td>0.058380</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.100364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Truppenteil</td>\n",
       "      <td>0.071744</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Rezeption</td>\n",
       "      <td>0.081854</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.464989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beimischen</td>\n",
       "      <td>0.084524</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.192625</td>\n",
       "      <td>0.307359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.983719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>zersetzen</td>\n",
       "      <td>0.125753</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.505880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Eintagsfliege</td>\n",
       "      <td>0.134180</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.660060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>verbauen</td>\n",
       "      <td>0.179274</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lyzeum</td>\n",
       "      <td>0.195752</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.126381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>vorweisen</td>\n",
       "      <td>0.347705</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.126837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word      dist      pval       fdr     shift  bin       mix\n",
       "26           Pachtzins  0.000000  1.000000  1.000000  0.000000  0.0  1.000000\n",
       "7           ausspannen  0.000000  1.000000  1.000000  0.706690  1.0  1.000000\n",
       "31           Sensation  0.001640  1.000000  1.000000  0.406144  1.0  0.998360\n",
       "40           vergÃ¶nnen  0.002014  0.000010  0.000021  0.071197  0.0  1.000000\n",
       "15           Frechheit  0.004143  0.131000  0.192625  0.070839  0.0  0.999202\n",
       "42           vorliegen  0.005621  0.974000  1.000000  0.190266  0.0  0.994379\n",
       "25             Ohrwurm  0.008051  0.000010  0.000021  0.832451  1.0  1.000000\n",
       "38  Unentschlossenheit  0.008196  1.000000  1.000000  0.000000  0.0  0.991804\n",
       "34               Titel  0.008204  0.994000  1.000000  0.393045  0.0  0.991796\n",
       "16                 FuÃŸ  0.012119  0.217000  0.293588  0.564633  0.0  0.996442\n",
       "44        weitgreifend  0.012358  0.000010  0.000021  0.000000  0.0  1.000000\n",
       "22            MiÃŸklang  0.014157  0.000010  0.000021  0.379723  1.0  1.000000\n",
       "5     aufrechterhalten  0.017321  0.000010  0.000021  0.036109  0.0  1.000000\n",
       "2           AckergerÃ¤t  0.017583  0.626000  0.799889  0.000000  0.0  0.985935\n",
       "13        Entscheidung  0.017885  0.000010  0.000021  0.141681  0.0  1.000000\n",
       "27              packen  0.018130  0.000010  0.000021  0.462253  1.0  1.000000\n",
       "29            Schmiere  0.021521  0.000010  0.000021  0.437671  1.0  1.000000\n",
       "17    Gesichtsausdruck  0.023743  1.000000  1.000000  0.077318  0.0  0.976257\n",
       "6       Ausnahmegesetz  0.023970  0.230000  0.302286  0.093138  0.0  0.992754\n",
       "33                Tier  0.026130  0.999000  1.000000  0.073466  0.0  0.973870\n",
       "30             Seminar  0.028230  0.000010  0.000021  0.064486  0.0  0.999999\n",
       "18         Knotenpunkt  0.029140  0.000010  0.000021  0.647627  1.0  0.999999\n",
       "23             Mulatte  0.032976  0.000010  0.000021  0.000000  0.0  0.999999\n",
       "37         Ã¼berspannen  0.033236  0.878947  1.000000  0.252066  1.0  0.966764\n",
       "1             Abgesang  0.033390  0.003210  0.005906  0.578548  1.0  0.999803\n",
       "32           Spielball  0.035748  0.000010  0.000021  0.103290  0.0  0.999999\n",
       "19          Kubikmeter  0.037032  0.000010  0.000021  0.000000  0.0  0.999999\n",
       "10         Einreichung  0.037602  0.031100  0.051093  0.000000  0.0  0.998079\n",
       "35       TragfÃ¤higkeit  0.039174  0.000010  0.000021  0.114694  0.0  0.999999\n",
       "3            Armenhaus  0.040294  0.000190  0.000380  0.519670  0.0  0.999985\n",
       "4         artikulieren  0.040389  0.998000  1.000000  0.615743  1.0  0.959611\n",
       "41        voranstellen  0.042848  0.000010  0.000021  0.124192  0.0  0.999999\n",
       "9              Dynamik  0.043546  0.006500  0.011244  0.578845  1.0  0.999510\n",
       "12              EngpaÃŸ  0.049950  0.006600  0.011244  0.819957  1.0  0.999438\n",
       "21          Manschette  0.051829  0.000010  0.000021  0.355802  1.0  0.999999\n",
       "24      NaturschÃ¶nheit  0.052340  0.000010  0.000021  0.071561  0.0  0.999999\n",
       "0            abgebrÃ¼ht  0.056371  0.151000  0.210485  0.832645  0.0  0.988135\n",
       "14           Festspiel  0.058380  0.084000  0.128800  0.100364  0.0  0.992481\n",
       "36         Truppenteil  0.071744  0.000010  0.000021  0.000000  0.0  0.999998\n",
       "28           Rezeption  0.081854  0.000010  0.000021  0.464989  1.0  0.999998\n",
       "8           beimischen  0.084524  0.134000  0.192625  0.307359  0.0  0.983719\n",
       "45           zersetzen  0.125753  0.058000  0.092000  0.505880  0.0  0.988431\n",
       "11       Eintagsfliege  0.134180  0.000220  0.000422  0.660060  0.0  0.999943\n",
       "39            verbauen  0.179274  0.000010  0.000021  0.578125  1.0  0.999996\n",
       "20              Lyzeum  0.195752  0.000010  0.000021  0.126381  0.0  0.999996\n",
       "43           vorweisen  0.347705  0.000010  0.000021  0.126837  0.0  0.999993"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr(method=\"spearman\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
